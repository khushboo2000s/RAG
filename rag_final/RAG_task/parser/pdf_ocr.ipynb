{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (1.24.10)\n",
      "Requirement already satisfied: pillow in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: easyocr in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.10 in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from pymupdf) (1.24.10)\n",
      "Requirement already satisfied: torch in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.5 in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (0.19.1)\n",
      "Requirement already satisfied: opencv-python-headless in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (4.10.0.84)\n",
      "Requirement already satisfied: scipy in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (1.14.1)\n",
      "Requirement already satisfied: numpy in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (1.26.4)\n",
      "Requirement already satisfied: scikit-image in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (0.24.0)\n",
      "Requirement already satisfied: python-bidi in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (0.6.0)\n",
      "Requirement already satisfied: PyYAML in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (6.0.2)\n",
      "Requirement already satisfied: Shapely in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (2.0.6)\n",
      "Requirement already satisfied: pyclipper in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (1.3.0.post5)\n",
      "Requirement already satisfied: ninja in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from easyocr) (1.11.1.1)\n",
      "Requirement already satisfied: filelock in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from torch->easyocr) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from torch->easyocr) (4.11.0)\n",
      "Requirement already satisfied: sympy in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from torch->easyocr) (1.13.2)\n",
      "Requirement already satisfied: networkx in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from torch->easyocr) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from torch->easyocr) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from torch->easyocr) (2024.9.0)\n",
      "Requirement already satisfied: imageio>=2.33 in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from scikit-image->easyocr) (2.35.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from scikit-image->easyocr) (2024.9.20)\n",
      "Requirement already satisfied: packaging>=21 in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from scikit-image->easyocr) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from scikit-image->easyocr) (0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from jinja2->torch->easyocr) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\genai_project\\retrieval augmented generation\\rag\\lib\\site-packages (from sympy->torch->easyocr) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pymupdf pillow easyocr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Convert PDF Pages to Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "\n",
    "def pdf_to_images(pdf_path):\n",
    "    images = []\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        for page_num in range(pdf.page_count):\n",
    "            page = pdf.load_page(page_num)\n",
    "            pix = page.get_pixmap()\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"D:/Genai_project/Retrieval Augmented Generation/rag_final/RAG_task/data_files/TDP-Document.pdf\"\n",
    "images = pdf_to_images(pdf_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Apply OCR Using EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: Structured TDP for Data Science and Generative Al Made by: Khushboo Gaur (TDP-AVML Intern}\n",
      "WEEK-I: Introduction and Setup: Data Science Foundations Overview of Data Science and Its Applications Data Science interdisciplinary field that combines statistical methods_ computer sciencc and domain expertise analyze andinterpret complex data. The primary goalisto uncovei patterns; generate insights, and drive decision-making processes Applications Data Science: Predictive Analytics: historical data Torecast future trends and behaviors. Customer Insights: Analyzes customer interactions and behaviors refine product offerings and optimize marketing strategies. Fraud Detection: Detects fraudulent activities by identifying anomalies transaction data. Healthcare Analytics: Enhances paticnt carc analyzing medical records and data treatment decisions: Basics of Data Analysis with Pandas Basic Operations: Loading Data: Import datasets into Pandas DataFrames for analysis using functions such as pd read Csv(). Filtering and Sorting: Extract and organize data based specific criteria mcthods like loc [ ], .iloc( - and sort_values(). Aggregating Data: Summarize datasets derive insights through operations such calculating mean sum count functions like mean( ) , - sum( ) , and groupby(). Key ' Urilzes Ruide usine using\n",
      "3. Assignments and Practical Setup Install Python = Required libraries Pandas, Numpy Matplotlib, Seacomi Gain familiarity with Python syntaxano Dasic daia structures Load dataset into a Pandas DataFrame;then perform basic data manipulation tasks such as filtering; sorting; and aggregating data_ Dataset used: Titanic csv Bankcsv Task: Practice loading datasets into Pandas DataFrame and perform basic data manipulation tasks (C.g . filtering; sorting). Pcsouice: Pandas_Data_Manipulation WEEK-2: Data Visualization and Exploratory Data Analysis (EDAL Overview: Data Visualization and Exploratory Data Analysis (EDA} are crucial components of the data analysis process This document outlines the fundamental principles of data visualization_ introduces key techniquesforexploratory data analysis, andhighlightsthe use of essential visualization libraries. Data Visualization Principles: Data visualization the graphical representation of information and data_ helps presenting complex data visually accessible format, making casicm comprehend patterns trends and oulicis Principles: Clarity: Ensure that the visualizations are clea and unambiguous Accuracy: Represent data accurately without misleading representations. Simplicity: Use simple designs to effectively communicate the intended messagc. Consistency: Maintain consistent design elements Gasici comparison and interpretation. Key\n",
      "Exploratory Data Analysis (EDA) Techniques: Exploratory Data Analysis involves summarizing and visualizing data understandits main characteristics before applying more formal statistical analyses. Key Techniques: Descriptive Statistics: Summarize data through measures suchac mnean mnedian mode standard deviation_ and variance. Data   Visualization: Utilize   plots and charts explore data distributions and relationships. Correlation Analysis: Examine the relationships beowween vamabies dentify potential correlations: Visualization Libraries: Matplotlib and Seaborn are two powerful Python libraries used for creating wide range of static; animatco_ interactive visualizations_ Matplotlib: comprenensie library for creating static, interactivc and animated visualizations in Python: It offers flexibility and customization options for varicty of plots- [pip install matplotlib] Scaborn: Built top of Matplotlib_ Seaborn provides high-level interface for drawing attracti0 ntormatie statistical graphics: simplifies complex visualizations with  more user-friendly API: [pip install seaborn] Assignments: Create Various Types of Plots Dataset- Restaurant_tips csv = iris.cSv data Scatter Plots: Create scatter plots - observe relationships betwcen two continuous wvariables\n",
      "Histograms: Generate histograms visualize the distribution of a single variable: Box Plots: Use box plots identify data spread and potential outlicrs: PCsOlice Various_types_of_plots Perform Exploratory Data Analysis (EDA) Dataset- Housing salc Dataset Analysis: Perform EDA given dataset summarze its  main characteristics. Visualization: Use Matplotlib and Seabom to create visual representations that highlight key insightsand trends in the data: Resouice: Exploratory Data Analysis (EDAI WEEK-3: Machine Learning Fundamentals Introduction to Machine Learning Concepts: Fundamentals of Machine Learning: Understanding the core principles of machine learning; including the definition, types lcarning; and its applications across various domains. Data Preparation: Importance of data preprocessing steps such as data cleaning; normalization feature selection to prepare datasets for machine learning models. Supervised and Unsupervised Learning Algorithms: Supervised Learning: Introduction supervised learning algorithms where the modelis trained on labeled data_\n",
      "Lincar Regression: Understanding linear relationsnips between variables and predicting ourcomcs Decision Trees: Learning abouttree structured models for classification and regression tasks Unsupervised Learning: Exploration of algorithms that deal with unlabeled data identify patterns and structurcs K-Means Clustering: Grouping data into clusters based on similarity . Principal Component Analysis (PCAJ: Reducing dimensionality of data while preserving its variance. Model Evaluation Techniques: pertormancc Metrics: Introduction key metrics uscd evaluating the performance maching learning models, including: Accuracy: Measure ofhow' often the model correctly predicts the outcome Precision and Recall: Understanding balance Detwcen positive predictions and actual positive cases scorc Combining precision and recall into single metric evaluate model performance under imbalanced data scenarios Cross-Validation: Techniqucs JS5853 moaCi oenomancc unscen data_ ensuring the model s generalizability- Assignments: Task-1: Implement simple machine learning Ilgorithms (e.g , linear regression Kemneans clusteringi Trom scratch using Python: Task-2: Evaluate the performance machine learning models using appropriate metrics (e.g-, accuracy; precision, recall)_ Dataset: boston_ house_prices Customer CSv\n",
      "implement egressiontor predicting numerica outcomes k-means clustering on = daraset segmenting the data into clusters Resouice: medns lincar_regression WEEK-4: Introduction to Natural Language Processing (NLP) DAY-1: Basics of NLP Introduction NLP: Definition: Natural Language Processing (NLP) is a field at the intersection - computer science, artificiale intelligence, linguistics, tocused On enabling computeis understand, intcrpret, and gencrate numan language Applications: Machine Translation Speech Recognition Chatbots Sentiment Analysis Text summarization Challenges: Language Ambiguity Context Understanding Sarcasm and Irony Tokenization: Splitting text into individual words Or tokens Stemming and Lemmatization: Reducing words thcir root form basc Cmma Part-of-Speech (POS) Tagging: Assigning grammatica categories (e.g-, noun,verb) to cachword Named Entity Recognition (NERJ: Identifying and classifying entities (0.& . names dates; locations) intext. Uinear\n",
      "Sentiment Analysis: Determining the sentiment expressed text (e.g positive_ negative, neutral)- Assignments: Use NLTK or spaCy to tokenize text data into words sentences Implement stemming and lemmatization eonchms Drocesstext data understanding the impact of each approach. Pcsouice: NLP task DAY-2: Text Preprocessing Techniques Text Cleaning: Text cleaning is the process of preparing and standardizing raw text data for analysis by removing correcting unvanted irrelevant inconsistenielements Remove noise from text data to improve analysis accuracy by ' eliminating special characters punctuation HTMLtags,and other irrelevant content: Steps: Lowercasing Removing Punctuation Removing Special Characters Removing Stopwords Tokenization Stemming and Lemmatization Removing whitespace Stopword Removal: Identify and remove common stopwords (e.g- \"the and thatdo not contribute significant meaning the text analysis. Build and maintain custoM list of stopwords tailorcd specific analysis nceds_\n",
      "Assignments: Task: Implementing; cleaning techniques expressions Building custom list of stopwords and removing them from text data_ Resouice: NLP_task DAY-3: Text Representation Bag-of-Words BoW) Model: Text [ represented as collection - words without order It is a simple and widely-used method for converting text data into numerica features that machine learning algorithms can process of words Text Data cute dog small 'smal dog , \"cute cal , 'cule dog' How the Bag-of-Words Model Works: using egular egaro Bo9 Cule\n",
      "\"This is how You ants tokenizer this- 'is\" how' you' get - ants Build & vocabulary over all documents aardvark ansterdam\" ants You- your\" zyxst' Sparse matrix encoding aardvark ants You zyxst Term Frequency Invcrsc Documcnt Frequency (TF-IDFJ: statistical measure that evaluates howimportantaword is to document in a llection: Term Frequency (TF): Measures how frequently word appears in document: Tnc idea that the more wword appears documentthe morc important mightbe_ Formula: Numbcr of tines the word appears in the document TF Total nuber of words in the documcnt Inveise Document Frequency (IDE): Measures how important Woro acioss documents the corpus Words thatappearin many documents are less uniquc and hence receive weights. get lovter\n",
      "Formula: Total number of documents IDF log Number of documents containing the word TE IDE Score: The TF-IDF score for. word is the product ofits TF and IDF values Formula: TF-IDF TF IDF Assignments: Task: Implementing BoW and TF IDF representations using scikit-learn Pcsouice: Bow_TF-IDF DAY-4: Introduction to NLP Libraries Overview = Popular NLP Libraries NLTK: comprehensive library for processing human language data Features: extensivc resource collection Text Processing Utilities Built-in Classifiers Functionalitics: Comprehensive Text Processing Educationa Resources Flexibility\n",
      "SpaCy: An industrial-strength NLP library with a focus performance and usability. Fcatures: High Performance NLP Pre-trained Models Pipeline Customization Functionalitics: Industrial-Strength Applications Deep Learning Integration Multilingual Processing TextBlob: Simplified text processing; built on top of NLTK and provides morc intuitive API Features: User-Friendly API Built on NLTK and Pattern Sentiment Analysis Functionalitics: Rapid Prototyping Text Classification Language Tools Gensim: Specialized topIc modeling and document similarity using unsupervised learning algorithms. Features: Specialized in Topic Modeling Document Similarity Analysis Memory-Efficient Processing Functionalitics: Big Data Text Processing Streaming and Incremental Training\n",
      "Extensibility Assignments: Task: Exploring NLTK and spaCyfor basic NLP tasks tokenization, POS tagging; and named entity recognition: Resource: tokenization_tagging_NER DAY-5: Sentiment Analysis Introduction Sentiment Analysis: Sentiment analysis the process of determining the emotiona tone behind series of Words uscd gain an understanding of the attitudes_ opinions_ and emotions expressed within an online mention Applications: Customer Feedback Analysis Social Media Monitoring Market Research Political Sentiment Analysis Healthcare Approaches: Ule\n",
      "Rule-Based Approaches: Relies on set of predefined linguistic rules sucnJs lexicons (lists of positive and negative words) and syntactic rules dercrminc sentiment: Machine Learning Approaches: Uses supervised earning lgorithms classifytext into sentiment categories based on fcaturcs extracted fromthe text. Common algorithms include Naive Bayes, Support Vector Machines (SVM); and Random Forest: Aspect ac Basc Approach Machine Learning-Based Approach Definition Uses predefined linguistic rules Uses supervised learning Igorithms such [exicons an syntactic traincd on labeled data- classify patterns_ determine sentiment sentiment Data No labeled data required; relies Requires labeled dataset for training Requirem predefined rules and (e-g. positive and negative examples). dictionaries Implemen Simple implement; involves More complexto implement; involves tation creating existing rules Tcatune cxtnction model sclection, Complexi dictionares_ and training: Accuracy Generally lower accuracy, Typically higher accuracy, as models especially with complex can learn from data and generalize t0 sentences sarcasm; and context- newexamples dependenttex uting\n",
      "Adaptabil Limited adaptability; rules must Highly adaptable; models can leam be manually updated Gxpanaco and improve from new data and can be tor nec contexts domains rctrained for different contexts Interpreta Highly interpretable; the Less interpretable; model decisions bility reasoning behind sentiment are basea on leared pattems; which decisions clcar and based on can be difficult to explain. Tues Resourcr Lol computational resources Higher computational resources Requirem needed; suitable for small-scale required, especially during training; ent applications  better suited for large-scale applications_ Handling Struggles with nuanced language Better at capturing nuances especially such sarcasm, idioms, and when using more sophisticated models Amances contex like deep learning Example sentiment analysis toolthat Amodeltrained on dataset 0T Usc Case usCS dictionary of positive and movie reviews - classify new reviews negative words t0 scorc as positive or negative _ sentiment Speed of Typically faster execution since it Slowverexecution during training; Execution involves straightforward mule however , prediction after training [ application generally fast  Flexibility Rigid; requires manual uodates Flexible; can automatically adaptto Tues handle new language new trends or jargon as the modelis trends domain specific jargon: retrained with updated data (orge\n",
      "Machine learning Lexicon based 1 Word Rule L 1 I 1 Word Rule Word Rule {ee[ure{ [  Machine learning algorithm Extract words; apply rules Classifier 1 Assignments: Task: Building simple sentiment analysis classificr using NLTK or scikit-leamn: Resouice: Sentiment_analysis\n",
      "Day 6-7: Project Assignment Objective: Apply Natural Language Processing (NLP) concepts develop modeltnat classifics text messages as either \"ham\" (non-spam) or \"spam: This projectfocuses on implementingand luating classification model that can effectivcly filter out unwanted messages Project Scope: The project involves creating machine lcarning pipelinc classily text messagcs The classification TaSk essential for various applications, including email filtering; SMS management content moderation systems Guidance: Continuous support providcd throughout proiCct ensure thorough understanding of key concepts such a8 preprocessing; feature extraction, model training; and evaluation_ Dataset: ham_spam CSv Resouice: NLP Project WEEK-5: Live Project Objective:\n",
      "Apply the Natural Language Tocessing (NLP COncepis eamec during the weck to rcal- world project: The goal is to implement sentiment analysis within_ banking-reclated proiect leveraging the tecnniques and models studied Project Scope: The Digi project cunenti ongoing   in the comoang Invoivcs analyzing customei interactions within the banking sector. The task focuses on performing sentiment analysis classify customer feedback; qucrics andresponses a8 Cither positive negativc This is crucial for understanding customer satisfaction and improving service quality: Guidance: Throughout the project; continuous support was provided to ensurc deep understanding of the methodologies applied_ The project involved using pre-trained sentiment analysis model, integrating custom based logic, and applying it to the banking domain. Project Implementation: Code Overvicw: The project Utilizes the distilbert-base uncaseo finetuned-sst-2-english model from Hugging Face Transformers, combined with custom rule based logic for specific short phrases commonly usC0 customer interactions. Pcsouice: Live jectiidigi_cloud) Document: Sentiment_document WEEK-6: Generative Al nule\n",
      "Generative Al Fundamentals Topics Covered Introduction to Gencrative Definition: Gencrative Al refers SUDSCi artificial intclligence that focuses creating new content, such as imagcs tcx audio, based on patterns lCarned from existing data Applications: Includes creative content generation data augmentation simulations. How Gencrative _ Works Core Mechanism: Gencrative _ moauis leam from large datasets tO unaeistand pattcrns and generate new; similar content Techniques include probabilistic models, neural networks and gencrative adversarialnctworks (GANs). Training Process: Involves training models on diverse datasets generate outputs that mimic the patterns and structures of the input data_ Hoiy Genamtivt Narks Generative Models Types: Generative Adversarial Networks (GANs): Consist of a generator and discriminator that compete to improve the quality = generated outputs. Variational Autoencoders (VAEs): Use probabilistic methods to generate e data samples from leamed distributions.\n",
      "Transformers: Models like GPT that use attention mechanisms generate text-based outputs_ Neural Networks and Generative Al Role Neural Networks: Ncural networks particularly decp learning models_ enhance the capabilities of generative Al by leaming complex patterns and generating high-quality content_ povanccments mcrovec arcnitecturcsang training techniques havc led t0 more sophisticated and realistic outputs Key Generative Al Models DALL-E: _ modcl developed by OpenAl that generates images from textual descriptions ChatGPT: _ conversationai devciodcc OpenAl that generates human-like text based on input prompts_ Bard: A language model developed by Google that provides natural language responses ana creative content generation_ Use Cases for Gencrative Al Content Creation: Automated generation of text, images andvideos for marketing and entertainment. Data Augmentation: Enhancing datasets with synthetic data for training machinc lcarning models. Personalization: Tailoring contentand expericnces based on individual preferences Benefits of Generative Al Creativity: Enables new forms creative expression and innovation Efficiency: Automates repetitive tasks andgenerates large volumes content quickly- Personalization: Provides customized expcricnces and recommendations_ Limitations of Generative Al\n",
      "Quality Control: Generated content ma} sometimesOC inaccurate quality- Ethical Concerns: Risks of misuse, including deepfakes and misinformation. Bias: Models may replicate biases Dresent the training data_ Examples of Generative Al Tools OpenAI s GPT-3 Google's BERT Runway ML CNN vs_ GANs: Differences Convolutional Neural Networks (CNNs): Primarily used for image recognition and processing by applying convolutional layers Generative Adversarial Networks (GANs): Used for generatingnew data samples througn juversaraltrainine between gencrator and discriminator. CNN Architecture: Convolubonjl Iax pacina Denac Hndulnci Olol 474 M2E\n",
      "GAN Architecture: Hctuiaee So7pk Drj necnmnant Culpl iFalt Bo7pk Diz Aspect Convolutional Ncural Gencrative Adversarial Networks (CNNs) Netnoric (GANs) Purpose Primarily used for image Designed generating recognition, classification, e data samples D} and segmentation. learning from existing data Architecture Comdosco convolutional Consists of two networks: layers_ pooling layers and Generator and fully connected layers. Discriminator. Main Components Convolu onal laycrs RcLU Generator (creates fake actkation pooling layers data) and Discriminator fully connected laycis. (evaluates data). Training Method Suderisco learning with Adversarial training where labeled data; uses Gencrator and backpropagation: Discriminator compete Primary Focus Feature extraction and Creating realistic and pattern recognition from diverse data samples. data_ Output Provides predictions Gencrates synthetic data classifications based on samples such as images _ input images text; audio Introduction to LangChain Topics Covered 4ol\n",
      "Introduction to LangChain Definition: LangChain is framework designeo for developing applications using languagc models_ providing tools for prompt engineering_ chaining, and integrating language models into various workflows Prompt Engineering and LLMs with LangChain Prompt Engineering: Techniques for designing effective prompts guide language models in generating desired outputs LLM Integration: Using LangChain connect language models to applications and workflows_ Core Components of LangChain Agents: Agents are entities within LangChain that perform tasks based on instructions provided by a language model They can execute actions, make decisions; and interact with various components of the system achieve specific goals- Chains: Chainsare sequences of operations 0r transformations that process inputs and gencrate outputs using language modcls They enable the creation complex workflows wherc different stages processing are linked together Tools: Tools refer to additional functionalities combonents integrated into LangChain applications Tnese Toois can perform specific tasks such as data retricval, processing or interaction with external systems Memory: Memory in LangChain allows the system retain and Utilize context or state across multiple intcractions language model. This eables more coherent and contextually relevant responses ovcr timc Retrievers: Retrievers are components that fetch relevant information from databases_ documents or other sources help providing the necessary context data required for generating accurate sponses performing tasks: Embeddings: Embeddings are numerical representations text Or othcr data that capture semantic mcanings andrelationships They are usedto convert Wifh They\n",
      "textual dara InIo format suitable processing Dy language models and other machinc lcarning lgorithms. Output Parsers: Output parsers are tools functions that interpret and format the outputs generated by anguage models They ensure that the generated text is structured and presented in a way that aligns with the application's requirements Vector Databases: Vector databases are specialized darabases designed storc and query vecto embeddings efficiently. enable fast and accurate retricval of information based on the similarity = vector rcpresentations Callback Handlers: Callback handlers are mnechanismstor managing and responding Events actions during the execution of language model tasks- They llow for the customization of behavior and interaction based on specific triggers conditions: How to Fine Tune the models: Preparation Thay '\n",
      "Define Your Task: Identify the specific application problem you wantthe model address_ Collect Data: Gather and label = relevant dataset for your task: Select a Model: Choosc pre-trained modci that aligns closely with your task 2 . Fine-Tuning Process Setup Environment: Ensure you have the necessany- softwarc and hardware f0r model training: Load Model . Tokenizer: Initializc the pre-trained modcl and its associated tokenizer. Prepare Data: Preprocess your dataset, including tokenization and formatting for compatibility with the model Configure Training: Set up training parameters such as batch size learning rate and thenumber of training epochs Train Model: Fine-tune the model on your specific dataset; adjusting its parameters based onyour task Evaluate Model: Assess the performance of the fine-tuned model using relevant metrics - ensure meets the desired objectives Post-Training Save Model: Store the finc-tuned model and tokenizer forfuture USC deployment: Deploy and Monitor: Deploy the modelin production environmentand continuously monitor its performance for any needed adjustments 0r improvcments Projects: Querying PDFs Using LangChain with OpenAI Objective: lement techniques extract and qucry information from PDF documents using LangChain and OpenAl models. Ince\n",
      "PCsOlice Query_paf Building PDF Document Question Answering LLMwith LangChain Objective: Develop language model capable of answering questions Dased on multiple PDF documents, allowing interactive querying and document analysis_ PCsOlice Wocumtent Q&A Resume Application Tracker Objective: Create an application trackand manage esume submissions utilizing language models categorize and analyze resumes Resouice: Resume_ATS WEEK-7: Fine-Tuning Models Exploring Hugging Face Models Fine Tune with Custom Data: Finc-Tuning Models Objective: Learn tcchniqucs for adag pre-trained language models to specific tasks domains through fine-tuning processes_ PCsOlice Finc_tuning Hugging Face Models: Hugging Face offers variety trained models for natural language cessing (NLP; tasks. Thesc models can be explored find one that best fits your specific needs can be finc-tunedfor customized tasks. Actions: Browse Model Hub: Visit the Hugging Face Model Hub - explorc available pre-trained models Models arc categorized by tasks Guch text classification, translation_ more Read Model Documentation: Review the documentation for each model uncerstand its capabilities, requirements; and howto use pting  Key '\n",
      "Select Model: Choosc modei based your task; dataset, and performance requirements Test the Model: Usc sample data to test the model's capabilities and determine its suitability for your specific application_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import numpy as np\n",
    "\n",
    "# Initialize EasyOCR reader (specify languages as needed)\n",
    "reader = easyocr.Reader(['en'])  # Add other languages in the list if needed\n",
    "\n",
    "def easyocr_extract(images):\n",
    "    extracted_text = \"\"\n",
    "    for img in images:\n",
    "        # Convert PIL image to NumPy array\n",
    "        img_np = np.array(img)\n",
    "        # Perform OCR on the image array\n",
    "        text = reader.readtext(img_np, detail=0)  # detail=0 for only text output\n",
    "        extracted_text += \" \".join(text) + \"\\n\"\n",
    "    return extracted_text\n",
    "\n",
    "# Extract text from images\n",
    "extracted_text = easyocr_extract(images)\n",
    "print(\"Extracted Text:\", extracted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Save Extracted Text to a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file = \"extracted_text.txt\"\n",
    "# with open(output_file, \"w\") as file:\n",
    "#     file.write(extracted_text)\n",
    "# print(f\"Extracted text saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply RAG Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 16:20:16,314 - INFO - Use pytorch device_name: cpu\n",
      "2024-10-28 16:20:16,320 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2024-10-28 16:20:19,823 - WARNING - Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "2024-10-28 16:23:56,108 - WARNING - retrievable is not a known attribute of class <class 'azure.search.documents.indexes.models._index.SearchField'> and will be ignored\n",
      "2024-10-28 16:23:56,145 - INFO - Request URL: 'https://gptkb-jcgzeo3krxxra.search.windows.net/indexes('rag-1')?api-version=REDACTED'\n",
      "Request method: 'PUT'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '1004'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Prefer': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': 'edb38389-951a-11ef-b592-c0b883fb6494'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.1 Python/3.10.14 (Windows-10-10.0.22631-SP0)'\n",
      "A body is sent with the request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: Structured TDP for Data Science and Generative Al Made by: Khushboo Gaur (TDP-AVML Intern}\n",
      "WEEK-I: Introduction and Setup: Data Science Foundations Overview of Data Science and Its Applications Data Science interdisciplinary field that combines statistical methods_ computer sciencc and domain expertise analyze andinterpret complex data. The primary goalisto uncovei patterns; generate insights, and drive decision-making processes Applications Data Science: Predictive Analytics: historical data Torecast future trends and behaviors. Customer Insights: Analyzes customer interactions and behaviors refine product offerings and optimize marketing strategies. Fraud Detection: Detects fraudulent activities by identifying anomalies transaction data. Healthcare Analytics: Enhances paticnt carc analyzing medical records and data treatment decisions: Basics of Data Analysis with Pandas Basic Operations: Loading Data: Import datasets into Pandas DataFrames for analysis using functions such as pd read Csv(). Filtering and Sorting: Extract and organize data based specific criteria mcthods like loc [ ], .iloc( - and sort_values(). Aggregating Data: Summarize datasets derive insights through operations such calculating mean sum count functions like mean( ) , - sum( ) , and groupby(). Key ' Urilzes Ruide usine using\n",
      "3. Assignments and Practical Setup Install Python = Required libraries Pandas, Numpy Matplotlib, Seacomi Gain familiarity with Python syntaxano Dasic daia structures Load dataset into a Pandas DataFrame;then perform basic data manipulation tasks such as filtering; sorting; and aggregating data_ Dataset used: Titanic csv Bankcsv Task: Practice loading datasets into Pandas DataFrame and perform basic data manipulation tasks (C.g . filtering; sorting). Pcsouice: Pandas_Data_Manipulation WEEK-2: Data Visualization and Exploratory Data Analysis (EDAL Overview: Data Visualization and Exploratory Data Analysis (EDA} are crucial components of the data analysis process This document outlines the fundamental principles of data visualization_ introduces key techniquesforexploratory data analysis, andhighlightsthe use of essential visualization libraries. Data Visualization Principles: Data visualization the graphical representation of information and data_ helps presenting complex data visually accessible format, making casicm comprehend patterns trends and oulicis Principles: Clarity: Ensure that the visualizations are clea and unambiguous Accuracy: Represent data accurately without misleading representations. Simplicity: Use simple designs to effectively communicate the intended messagc. Consistency: Maintain consistent design elements Gasici comparison and interpretation. Key\n",
      "Exploratory Data Analysis (EDA) Techniques: Exploratory Data Analysis involves summarizing and visualizing data understandits main characteristics before applying more formal statistical analyses. Key Techniques: Descriptive Statistics: Summarize data through measures suchac mnean mnedian mode standard deviation_ and variance. Data   Visualization: Utilize   plots and charts explore data distributions and relationships. Correlation Analysis: Examine the relationships beowween vamabies dentify potential correlations: Visualization Libraries: Matplotlib and Seaborn are two powerful Python libraries used for creating wide range of static; animatco_ interactive visualizations_ Matplotlib: comprenensie library for creating static, interactivc and animated visualizations in Python: It offers flexibility and customization options for varicty of plots- [pip install matplotlib] Scaborn: Built top of Matplotlib_ Seaborn provides high-level interface for drawing attracti0 ntormatie statistical graphics: simplifies complex visualizations with  more user-friendly API: [pip install seaborn] Assignments: Create Various Types of Plots Dataset- Restaurant_tips csv = iris.cSv data Scatter Plots: Create scatter plots - observe relationships betwcen two continuous wvariables\n",
      "Histograms: Generate histograms visualize the distribution of a single variable: Box Plots: Use box plots identify data spread and potential outlicrs: PCsOlice Various_types_of_plots Perform Exploratory Data Analysis (EDA) Dataset- Housing salc Dataset Analysis: Perform EDA given dataset summarze its  main characteristics. Visualization: Use Matplotlib and Seabom to create visual representations that highlight key insightsand trends in the data: Resouice: Exploratory Data Analysis (EDAI WEEK-3: Machine Learning Fundamentals Introduction to Machine Learning Concepts: Fundamentals of Machine Learning: Understanding the core principles of machine learning; including the definition, types lcarning; and its applications across various domains. Data Preparation: Importance of data preprocessing steps such as data cleaning; normalization feature selection to prepare datasets for machine learning models. Supervised and Unsupervised Learning Algorithms: Supervised Learning: Introduction supervised learning algorithms where the modelis trained on labeled data_\n",
      "Lincar Regression: Understanding linear relationsnips between variables and predicting ourcomcs Decision Trees: Learning abouttree structured models for classification and regression tasks Unsupervised Learning: Exploration of algorithms that deal with unlabeled data identify patterns and structurcs K-Means Clustering: Grouping data into clusters based on similarity . Principal Component Analysis (PCAJ: Reducing dimensionality of data while preserving its variance. Model Evaluation Techniques: pertormancc Metrics: Introduction key metrics uscd evaluating the performance maching learning models, including: Accuracy: Measure ofhow' often the model correctly predicts the outcome Precision and Recall: Understanding balance Detwcen positive predictions and actual positive cases scorc Combining precision and recall into single metric evaluate model performance under imbalanced data scenarios Cross-Validation: Techniqucs JS5853 moaCi oenomancc unscen data_ ensuring the model s generalizability- Assignments: Task-1: Implement simple machine learning Ilgorithms (e.g , linear regression Kemneans clusteringi Trom scratch using Python: Task-2: Evaluate the performance machine learning models using appropriate metrics (e.g-, accuracy; precision, recall)_ Dataset: boston_ house_prices Customer CSv\n",
      "implement egressiontor predicting numerica outcomes k-means clustering on = daraset segmenting the data into clusters Resouice: medns lincar_regression WEEK-4: Introduction to Natural Language Processing (NLP) DAY-1: Basics of NLP Introduction NLP: Definition: Natural Language Processing (NLP) is a field at the intersection - computer science, artificiale intelligence, linguistics, tocused On enabling computeis understand, intcrpret, and gencrate numan language Applications: Machine Translation Speech Recognition Chatbots Sentiment Analysis Text summarization Challenges: Language Ambiguity Context Understanding Sarcasm and Irony Tokenization: Splitting text into individual words Or tokens Stemming and Lemmatization: Reducing words thcir root form basc Cmma Part-of-Speech (POS) Tagging: Assigning grammatica categories (e.g-, noun,verb) to cachword Named Entity Recognition (NERJ: Identifying and classifying entities (0.& . names dates; locations) intext. Uinear\n",
      "Sentiment Analysis: Determining the sentiment expressed text (e.g positive_ negative, neutral)- Assignments: Use NLTK or spaCy to tokenize text data into words sentences Implement stemming and lemmatization eonchms Drocesstext data understanding the impact of each approach. Pcsouice: NLP task DAY-2: Text Preprocessing Techniques Text Cleaning: Text cleaning is the process of preparing and standardizing raw text data for analysis by removing correcting unvanted irrelevant inconsistenielements Remove noise from text data to improve analysis accuracy by ' eliminating special characters punctuation HTMLtags,and other irrelevant content: Steps: Lowercasing Removing Punctuation Removing Special Characters Removing Stopwords Tokenization Stemming and Lemmatization Removing whitespace Stopword Removal: Identify and remove common stopwords (e.g- \"the and thatdo not contribute significant meaning the text analysis. Build and maintain custoM list of stopwords tailorcd specific analysis nceds_\n",
      "Assignments: Task: Implementing; cleaning techniques expressions Building custom list of stopwords and removing them from text data_ Resouice: NLP_task DAY-3: Text Representation Bag-of-Words BoW) Model: Text [ represented as collection - words without order It is a simple and widely-used method for converting text data into numerica features that machine learning algorithms can process of words Text Data cute dog small 'smal dog , \"cute cal , 'cule dog' How the Bag-of-Words Model Works: using egular egaro Bo9 Cule\n",
      "\"This is how You ants tokenizer this- 'is\" how' you' get - ants Build & vocabulary over all documents aardvark ansterdam\" ants You- your\" zyxst' Sparse matrix encoding aardvark ants You zyxst Term Frequency Invcrsc Documcnt Frequency (TF-IDFJ: statistical measure that evaluates howimportantaword is to document in a llection: Term Frequency (TF): Measures how frequently word appears in document: Tnc idea that the more wword appears documentthe morc important mightbe_ Formula: Numbcr of tines the word appears in the document TF Total nuber of words in the documcnt Inveise Document Frequency (IDE): Measures how important Woro acioss documents the corpus Words thatappearin many documents are less uniquc and hence receive weights. get lovter\n",
      "Formula: Total number of documents IDF log Number of documents containing the word TE IDE Score: The TF-IDF score for. word is the product ofits TF and IDF values Formula: TF-IDF TF IDF Assignments: Task: Implementing BoW and TF IDF representations using scikit-learn Pcsouice: Bow_TF-IDF DAY-4: Introduction to NLP Libraries Overview = Popular NLP Libraries NLTK: comprehensive library for processing human language data Features: extensivc resource collection Text Processing Utilities Built-in Classifiers Functionalitics: Comprehensive Text Processing Educationa Resources Flexibility\n",
      "SpaCy: An industrial-strength NLP library with a focus performance and usability. Fcatures: High Performance NLP Pre-trained Models Pipeline Customization Functionalitics: Industrial-Strength Applications Deep Learning Integration Multilingual Processing TextBlob: Simplified text processing; built on top of NLTK and provides morc intuitive API Features: User-Friendly API Built on NLTK and Pattern Sentiment Analysis Functionalitics: Rapid Prototyping Text Classification Language Tools Gensim: Specialized topIc modeling and document similarity using unsupervised learning algorithms. Features: Specialized in Topic Modeling Document Similarity Analysis Memory-Efficient Processing Functionalitics: Big Data Text Processing Streaming and Incremental Training\n",
      "Extensibility Assignments: Task: Exploring NLTK and spaCyfor basic NLP tasks tokenization, POS tagging; and named entity recognition: Resource: tokenization_tagging_NER DAY-5: Sentiment Analysis Introduction Sentiment Analysis: Sentiment analysis the process of determining the emotiona tone behind series of Words uscd gain an understanding of the attitudes_ opinions_ and emotions expressed within an online mention Applications: Customer Feedback Analysis Social Media Monitoring Market Research Political Sentiment Analysis Healthcare Approaches: Ule\n",
      "Rule-Based Approaches: Relies on set of predefined linguistic rules sucnJs lexicons (lists of positive and negative words) and syntactic rules dercrminc sentiment: Machine Learning Approaches: Uses supervised earning lgorithms classifytext into sentiment categories based on fcaturcs extracted fromthe text. Common algorithms include Naive Bayes, Support Vector Machines (SVM); and Random Forest: Aspect ac Basc Approach Machine Learning-Based Approach Definition Uses predefined linguistic rules Uses supervised learning Igorithms such [exicons an syntactic traincd on labeled data- classify patterns_ determine sentiment sentiment Data No labeled data required; relies Requires labeled dataset for training Requirem predefined rules and (e-g. positive and negative examples). dictionaries Implemen Simple implement; involves More complexto implement; involves tation creating existing rules Tcatune cxtnction model sclection, Complexi dictionares_ and training: Accuracy Generally lower accuracy, Typically higher accuracy, as models especially with complex can learn from data and generalize t0 sentences sarcasm; and context- newexamples dependenttex uting\n",
      "Adaptabil Limited adaptability; rules must Highly adaptable; models can leam be manually updated Gxpanaco and improve from new data and can be tor nec contexts domains rctrained for different contexts Interpreta Highly interpretable; the Less interpretable; model decisions bility reasoning behind sentiment are basea on leared pattems; which decisions clcar and based on can be difficult to explain. Tues Resourcr Lol computational resources Higher computational resources Requirem needed; suitable for small-scale required, especially during training; ent applications  better suited for large-scale applications_ Handling Struggles with nuanced language Better at capturing nuances especially such sarcasm, idioms, and when using more sophisticated models Amances contex like deep learning Example sentiment analysis toolthat Amodeltrained on dataset 0T Usc Case usCS dictionary of positive and movie reviews - classify new reviews negative words t0 scorc as positive or negative _ sentiment Speed of Typically faster execution since it Slowverexecution during training; Execution involves straightforward mule however , prediction after training [ application generally fast  Flexibility Rigid; requires manual uodates Flexible; can automatically adaptto Tues handle new language new trends or jargon as the modelis trends domain specific jargon: retrained with updated data (orge\n",
      "Machine learning Lexicon based 1 Word Rule L 1 I 1 Word Rule Word Rule {ee[ure{ [  Machine learning algorithm Extract words; apply rules Classifier 1 Assignments: Task: Building simple sentiment analysis classificr using NLTK or scikit-leamn: Resouice: Sentiment_analysis\n",
      "Day 6-7: Project Assignment Objective: Apply Natural Language Processing (NLP) concepts develop modeltnat classifics text messages as either \"ham\" (non-spam) or \"spam: This projectfocuses on implementingand luating classification model that can effectivcly filter out unwanted messages Project Scope: The project involves creating machine lcarning pipelinc classily text messagcs The classification TaSk essential for various applications, including email filtering; SMS management content moderation systems Guidance: Continuous support providcd throughout proiCct ensure thorough understanding of key concepts such a8 preprocessing; feature extraction, model training; and evaluation_ Dataset: ham_spam CSv Resouice: NLP Project WEEK-5: Live Project Objective:\n",
      "Apply the Natural Language Tocessing (NLP COncepis eamec during the weck to rcal- world project: The goal is to implement sentiment analysis within_ banking-reclated proiect leveraging the tecnniques and models studied Project Scope: The Digi project cunenti ongoing   in the comoang Invoivcs analyzing customei interactions within the banking sector. The task focuses on performing sentiment analysis classify customer feedback; qucrics andresponses a8 Cither positive negativc This is crucial for understanding customer satisfaction and improving service quality: Guidance: Throughout the project; continuous support was provided to ensurc deep understanding of the methodologies applied_ The project involved using pre-trained sentiment analysis model, integrating custom based logic, and applying it to the banking domain. Project Implementation: Code Overvicw: The project Utilizes the distilbert-base uncaseo finetuned-sst-2-english model from Hugging Face Transformers, combined with custom rule based logic for specific short phrases commonly usC0 customer interactions. Pcsouice: Live jectiidigi_cloud) Document: Sentiment_document WEEK-6: Generative Al nule\n",
      "Generative Al Fundamentals Topics Covered Introduction to Gencrative Definition: Gencrative Al refers SUDSCi artificial intclligence that focuses creating new content, such as imagcs tcx audio, based on patterns lCarned from existing data Applications: Includes creative content generation data augmentation simulations. How Gencrative _ Works Core Mechanism: Gencrative _ moauis leam from large datasets tO unaeistand pattcrns and generate new; similar content Techniques include probabilistic models, neural networks and gencrative adversarialnctworks (GANs). Training Process: Involves training models on diverse datasets generate outputs that mimic the patterns and structures of the input data_ Hoiy Genamtivt Narks Generative Models Types: Generative Adversarial Networks (GANs): Consist of a generator and discriminator that compete to improve the quality = generated outputs. Variational Autoencoders (VAEs): Use probabilistic methods to generate e data samples from leamed distributions.\n",
      "Transformers: Models like GPT that use attention mechanisms generate text-based outputs_ Neural Networks and Generative Al Role Neural Networks: Ncural networks particularly decp learning models_ enhance the capabilities of generative Al by leaming complex patterns and generating high-quality content_ povanccments mcrovec arcnitecturcsang training techniques havc led t0 more sophisticated and realistic outputs Key Generative Al Models DALL-E: _ modcl developed by OpenAl that generates images from textual descriptions ChatGPT: _ conversationai devciodcc OpenAl that generates human-like text based on input prompts_ Bard: A language model developed by Google that provides natural language responses ana creative content generation_ Use Cases for Gencrative Al Content Creation: Automated generation of text, images andvideos for marketing and entertainment. Data Augmentation: Enhancing datasets with synthetic data for training machinc lcarning models. Personalization: Tailoring contentand expericnces based on individual preferences Benefits of Generative Al Creativity: Enables new forms creative expression and innovation Efficiency: Automates repetitive tasks andgenerates large volumes content quickly- Personalization: Provides customized expcricnces and recommendations_ Limitations of Generative Al\n",
      "Quality Control: Generated content ma} sometimesOC inaccurate quality- Ethical Concerns: Risks of misuse, including deepfakes and misinformation. Bias: Models may replicate biases Dresent the training data_ Examples of Generative Al Tools OpenAI s GPT-3 Google's BERT Runway ML CNN vs_ GANs: Differences Convolutional Neural Networks (CNNs): Primarily used for image recognition and processing by applying convolutional layers Generative Adversarial Networks (GANs): Used for generatingnew data samples througn juversaraltrainine between gencrator and discriminator. CNN Architecture: Convolubonjl Iax pacina Denac Hndulnci Olol 474 M2E\n",
      "GAN Architecture: Hctuiaee So7pk Drj necnmnant Culpl iFalt Bo7pk Diz Aspect Convolutional Ncural Gencrative Adversarial Networks (CNNs) Netnoric (GANs) Purpose Primarily used for image Designed generating recognition, classification, e data samples D} and segmentation. learning from existing data Architecture Comdosco convolutional Consists of two networks: layers_ pooling layers and Generator and fully connected layers. Discriminator. Main Components Convolu onal laycrs RcLU Generator (creates fake actkation pooling layers data) and Discriminator fully connected laycis. (evaluates data). Training Method Suderisco learning with Adversarial training where labeled data; uses Gencrator and backpropagation: Discriminator compete Primary Focus Feature extraction and Creating realistic and pattern recognition from diverse data samples. data_ Output Provides predictions Gencrates synthetic data classifications based on samples such as images _ input images text; audio Introduction to LangChain Topics Covered 4ol\n",
      "Introduction to LangChain Definition: LangChain is framework designeo for developing applications using languagc models_ providing tools for prompt engineering_ chaining, and integrating language models into various workflows Prompt Engineering and LLMs with LangChain Prompt Engineering: Techniques for designing effective prompts guide language models in generating desired outputs LLM Integration: Using LangChain connect language models to applications and workflows_ Core Components of LangChain Agents: Agents are entities within LangChain that perform tasks based on instructions provided by a language model They can execute actions, make decisions; and interact with various components of the system achieve specific goals- Chains: Chainsare sequences of operations 0r transformations that process inputs and gencrate outputs using language modcls They enable the creation complex workflows wherc different stages processing are linked together Tools: Tools refer to additional functionalities combonents integrated into LangChain applications Tnese Toois can perform specific tasks such as data retricval, processing or interaction with external systems Memory: Memory in LangChain allows the system retain and Utilize context or state across multiple intcractions language model. This eables more coherent and contextually relevant responses ovcr timc Retrievers: Retrievers are components that fetch relevant information from databases_ documents or other sources help providing the necessary context data required for generating accurate sponses performing tasks: Embeddings: Embeddings are numerical representations text Or othcr data that capture semantic mcanings andrelationships They are usedto convert Wifh They\n",
      "textual dara InIo format suitable processing Dy language models and other machinc lcarning lgorithms. Output Parsers: Output parsers are tools functions that interpret and format the outputs generated by anguage models They ensure that the generated text is structured and presented in a way that aligns with the application's requirements Vector Databases: Vector databases are specialized darabases designed storc and query vecto embeddings efficiently. enable fast and accurate retricval of information based on the similarity = vector rcpresentations Callback Handlers: Callback handlers are mnechanismstor managing and responding Events actions during the execution of language model tasks- They llow for the customization of behavior and interaction based on specific triggers conditions: How to Fine Tune the models: Preparation Thay '\n",
      "Define Your Task: Identify the specific application problem you wantthe model address_ Collect Data: Gather and label = relevant dataset for your task: Select a Model: Choosc pre-trained modci that aligns closely with your task 2 . Fine-Tuning Process Setup Environment: Ensure you have the necessany- softwarc and hardware f0r model training: Load Model . Tokenizer: Initializc the pre-trained modcl and its associated tokenizer. Prepare Data: Preprocess your dataset, including tokenization and formatting for compatibility with the model Configure Training: Set up training parameters such as batch size learning rate and thenumber of training epochs Train Model: Fine-tune the model on your specific dataset; adjusting its parameters based onyour task Evaluate Model: Assess the performance of the fine-tuned model using relevant metrics - ensure meets the desired objectives Post-Training Save Model: Store the finc-tuned model and tokenizer forfuture USC deployment: Deploy and Monitor: Deploy the modelin production environmentand continuously monitor its performance for any needed adjustments 0r improvcments Projects: Querying PDFs Using LangChain with OpenAI Objective: lement techniques extract and qucry information from PDF documents using LangChain and OpenAl models. Ince\n",
      "PCsOlice Query_paf Building PDF Document Question Answering LLMwith LangChain Objective: Develop language model capable of answering questions Dased on multiple PDF documents, allowing interactive querying and document analysis_ PCsOlice Wocumtent Q&A Resume Application Tracker Objective: Create an application trackand manage esume submissions utilizing language models categorize and analyze resumes Resouice: Resume_ATS WEEK-7: Fine-Tuning Models Exploring Hugging Face Models Fine Tune with Custom Data: Finc-Tuning Models Objective: Learn tcchniqucs for adag pre-trained language models to specific tasks domains through fine-tuning processes_ PCsOlice Finc_tuning Hugging Face Models: Hugging Face offers variety trained models for natural language cessing (NLP; tasks. Thesc models can be explored find one that best fits your specific needs can be finc-tunedfor customized tasks. Actions: Browse Model Hub: Visit the Hugging Face Model Hub - explorc available pre-trained models Models arc categorized by tasks Guch text classification, translation_ more Read Model Documentation: Review the documentation for each model uncerstand its capabilities, requirements; and howto use pting  Key '\n",
      "Select Model: Choosc modei based your task; dataset, and performance requirements Test the Model: Usc sample data to test the model's capabilities and determine its suitability for your specific application_\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 16:23:57,296 - INFO - Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'edb38389-951a-11ef-b592-c0b883fb6494'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Mon, 28 Oct 2024 10:53:58 GMT'\n",
      "2024-10-28 16:23:57,313 - INFO - Index created or updated. Result: {'additional_properties': {}, 'name': 'rag-1', 'fields': [<azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B59C0>, <azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B4E50>, <azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B4DC0>, <azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B61A0>], 'scoring_profiles': [], 'default_scoring_profile': None, 'cors_options': None, 'suggesters': [], 'analyzers': None, 'tokenizers': None, 'token_filters': [], 'char_filters': [], 'encryption_key': None, 'similarity': <azure.search.documents.indexes._generated.models._models_py3.BM25SimilarityAlgorithm object at 0x00000207A75B6980>, 'semantic_search': <azure.search.documents.indexes._generated.models._models_py3.SemanticSearch object at 0x00000207A75B47C0>, 'vector_search': <azure.search.documents.indexes._generated.models._models_py3.VectorSearch object at 0x00000207A75B6200>, 'e_tag': None}\n",
      "2024-10-28 16:23:57,317 - INFO - Request URL: 'https://gptkb-jcgzeo3krxxra.search.windows.net/indexes('rag-1')?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': 'ee66bc38-951a-11ef-9bce-c0b883fb6494'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.1 Python/3.10.14 (Windows-10-10.0.22631-SP0)'\n",
      "No body was attached to the request\n",
      "2024-10-28 16:23:57,573 - INFO - Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'ETag': '\"0x8DCE843C374C0F0\"'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'ee66bc38-951a-11ef-9bce-c0b883fb6494'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Mon, 28 Oct 2024 10:53:58 GMT'\n",
      "2024-10-28 16:23:57,578 - INFO - Index retrieved: {'additional_properties': {}, 'name': 'rag-1', 'fields': [<azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B5E40>, <azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B4310>, <azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B41F0>, <azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B4130>], 'scoring_profiles': [], 'default_scoring_profile': None, 'cors_options': None, 'suggesters': [], 'analyzers': None, 'tokenizers': None, 'token_filters': [], 'char_filters': [], 'encryption_key': None, 'similarity': <azure.search.documents.indexes._generated.models._models_py3.BM25SimilarityAlgorithm object at 0x00000207A75B7FD0>, 'semantic_search': <azure.search.documents.indexes._generated.models._models_py3.SemanticSearch object at 0x00000207A75B6C20>, 'vector_search': <azure.search.documents.indexes._generated.models._models_py3.VectorSearch object at 0x00000207A75B4B20>, 'e_tag': '\"0x8DCE843C374C0F0\"'}\n",
      "2024-10-28 16:23:57,591 - INFO - Processing extracted text...\n",
      "2024-10-28 16:23:57,725 - INFO - Number of chunks created: 6\n",
      "2024-10-28 16:23:57,728 - INFO - Generating embeddings using local model...\n",
      "2024-10-28 16:23:57,730 - ERROR - Error generating embeddings using local model: encode() argument 'encoding' must be str, not list\n",
      "2024-10-28 16:23:57,731 - INFO - Generated embeddings for 0 chunks.\n",
      "2024-10-28 16:23:57,735 - INFO - Processing completed for the extracted text.\n",
      "2024-10-28 16:23:57,737 - ERROR - No data was processed. Check input files or processing logic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: {'additional_properties': {}, 'name': 'rag-1', 'fields': [<azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B5E40>, <azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B4310>, <azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B41F0>, <azure.search.documents.indexes.models._index.SearchField object at 0x00000207A75B4130>], 'scoring_profiles': [], 'default_scoring_profile': None, 'cors_options': None, 'suggesters': [], 'analyzers': None, 'tokenizers': None, 'token_filters': [], 'char_filters': [], 'encryption_key': None, 'similarity': <azure.search.documents.indexes._generated.models._models_py3.BM25SimilarityAlgorithm object at 0x00000207A75B7FD0>, 'semantic_search': <azure.search.documents.indexes._generated.models._models_py3.SemanticSearch object at 0x00000207A75B6C20>, 'vector_search': <azure.search.documents.indexes._generated.models._models_py3.VectorSearch object at 0x00000207A75B4B20>, 'e_tag': '\"0x8DCE843C374C0F0\"'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import AzureOpenAI\n",
    "from config import Config\n",
    "import re\n",
    "from search_utilities import create_search_index, upload_documents\n",
    "import easyocr\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the data folder path and chunk settings from the .env file\n",
    "data_folder_path = os.getenv('DATA_FOLDER_PATH')\n",
    "chunk_size = int(os.getenv('CHUNK_SIZE', 1000))  # Default chunk size is 1000 characters\n",
    "chunk_overlap = int(os.getenv('CHUNK_OVERLAP', 200))  # Default overlap is 200 characters\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Initialize Azure OpenAI client if in cloud approach\n",
    "if config.APPROACH == 'cloud':\n",
    "    client = AzureOpenAI(azure_endpoint=os.getenv(\"OPENAI_API_BASE\"),\n",
    "                         api_key=\"bbc851a28be648d88779cd1e3de2feee\",\n",
    "                         api_version='2024-02-15-preview')\n",
    "else:\n",
    "    # For on-premises, initialize the sentence transformer model\n",
    "    model_name = os.getenv('EMBEDDING_MODEL_NAME_ON_PREM', 'sentence-transformers/all-mpnet-base-v2')\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])  # Add other languages in the list if needed\n",
    "\n",
    "# Function to extract text from images using EasyOCR\n",
    "def easyocr_extract(images):\n",
    "    extracted_text = \"\"\n",
    "    for img in images:\n",
    "        # Convert PIL image to NumPy array\n",
    "        img_np = np.array(img)\n",
    "        # Perform OCR on the image array\n",
    "        text = reader.readtext(img_np, detail=0)  # detail=0 for only text output\n",
    "        extracted_text += \" \".join(text) + \"\\n\"\n",
    "    return extracted_text\n",
    "\n",
    "# Class for recursive text chunking\n",
    "class RecursiveChunker:\n",
    "    def __init__(self, chunk_size: int, chunk_overlap: int):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def chunk_documents(self, docs: List[Document]) -> List[Document]:\n",
    "        if not docs:\n",
    "            logging.warning(\"No documents provided for chunking.\")\n",
    "            return []\n",
    "\n",
    "        # Check if the method exists and is available\n",
    "        try:\n",
    "            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                encoding_name=\"cl100k_base\",\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "        except AttributeError as e:\n",
    "            logging.error(f\"Error initializing text splitter: {e}\")\n",
    "            return []\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        chunked_docs = splitter.split_documents(docs)\n",
    "        return chunked_docs\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(text_list, model=\"embedding\"):\n",
    "    embeddings = []\n",
    "    if config.APPROACH == 'cloud':\n",
    "        logging.info('Generating embeddings using cloud model...')\n",
    "        for text in text_list:\n",
    "            try:\n",
    "                embedding = client.embeddings.create(\n",
    "                    input=[text],\n",
    "                    model=model  # Use the model parameter as \"embedding\"\n",
    "                ).data[0].embedding\n",
    "                embeddings.append(embedding)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error generating embedding for text: {e}\")\n",
    "    else:\n",
    "        logging.info('Generating embeddings using local model...')\n",
    "        try:\n",
    "            embeddings = model.encode(text_list).tolist()  # Ensure embeddings are list\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating embeddings using local model: {e}\")\n",
    "    return embeddings\n",
    "\n",
    "# Function to handle extracted text, chunking, and embeddings\n",
    "def process_extracted_text(extracted_text: str):\n",
    "    # Create the search index at the beginning\n",
    "    create_search_index()\n",
    "\n",
    "    chunker = RecursiveChunker(chunk_size, chunk_overlap)\n",
    "    processed_documents = []  # Create a list to hold processed documents\n",
    "\n",
    "    logging.info(\"Processing extracted text...\")\n",
    "\n",
    "    # Convert the text into a Document object\n",
    "    documents = [Document(page_content=extracted_text)]\n",
    "\n",
    "    # Chunk the documents\n",
    "    chunks = chunker.chunk_documents(documents)\n",
    "\n",
    "    logging.info(f\"Number of chunks created: {len(chunks)}\")\n",
    "\n",
    "    if not chunks:\n",
    "        logging.warning(\"No chunks created from the extracted text.\")\n",
    "        return []\n",
    "\n",
    "    # Extract text from chunks\n",
    "    chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "    # Generate embeddings for each chunk\n",
    "    embeddings = generate_embeddings(chunk_texts)\n",
    "    logging.info(f\"Generated embeddings for {len(embeddings)} chunks.\")\n",
    "\n",
    "    # Sanitize filename and create a document ID\n",
    "    document_id = \"extracted_text_document\"  # You can change this ID as needed\n",
    "\n",
    "    # Process each chunk and create a document structure\n",
    "    for chunk_text, embedding in zip(chunk_texts, embeddings):\n",
    "        # Validate embeddings\n",
    "        if not (isinstance(embedding, list) and all(isinstance(x, float) for x in embedding)):\n",
    "            logging.error(f\"Embedding is not a list of floats for chunk.\")\n",
    "            continue\n",
    "\n",
    "        # Create a document dictionary\n",
    "        document = {\n",
    "            \"id\": document_id,\n",
    "            \"title\": \"Extracted Text Document\",\n",
    "            \"content\": chunk_text,\n",
    "            \"contentVector\": embedding\n",
    "        }\n",
    "\n",
    "        # Append processed document to the list\n",
    "        processed_documents.append(document)\n",
    "\n",
    "        # Upload or index the document based on the approach\n",
    "        if config.APPROACH == 'cloud':\n",
    "            upload_documents([document])\n",
    "            logging.info(\"Uploaded document to Azure Search.\")\n",
    "        else:\n",
    "            index_on_prem_faiss([embedding], [document_id])\n",
    "            logging.info(\"Uploaded embedding to FAISS.\")\n",
    "\n",
    "    logging.info(\"Processing completed for the extracted text.\")\n",
    "    \n",
    "    return processed_documents  # Return the processed documents list\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Convert PDF to images\n",
    "    pdf_path = \"D:/Genai_project/Retrieval Augmented Generation/rag_final/RAG_task/data_files/TDP-Document.pdf\"\n",
    "    images = pdf_to_images(pdf_path)\n",
    "\n",
    "    # Step 2: Extract text from images using EasyOCR\n",
    "    extracted_text = easyocr_extract(images)\n",
    "    print(\"Extracted Text:\", extracted_text)  # Optional: Print the extracted text for verification\n",
    "\n",
    "    # Step 3: Process the extracted text for chunking and embeddings\n",
    "    processed_data = process_extracted_text(extracted_text)\n",
    "\n",
    "    if processed_data:\n",
    "        logging.info(\"Chunks and embeddings are created.\")\n",
    "    else:\n",
    "        logging.error(\"No data was processed. Check input files or processing logic.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
